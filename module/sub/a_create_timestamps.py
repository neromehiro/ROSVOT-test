# module/sub/a_create_timestamps.py
########## [Script Entry Point] ãƒ‘ã‚¹è¨­å®šãƒ–ãƒ­ãƒƒã‚¯ - é–‹å§‹ ##########
import sys
import os

if __name__ == "__main__" and __package__ is None:
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’sys.pathã«è¿½åŠ 
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(script_dir, "../../")) # 2éšå±¤ä¸‹ã®å ´åˆ
    sys.path.insert(0, project_root)
########## [Script Entry Point] ãƒ‘ã‚¹è¨­å®šãƒ–ãƒ­ãƒƒã‚¯ - çµ‚äº† ##########

import json
import re
import subprocess
import shutil
import tempfile
from pathlib import Path
from typing import List, Dict, Tuple, Optional

# ã‹ãªâ†’SOFAè¾æ›¸ãƒ­ãƒ¼ãƒå­—å¤‰æ›ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°
YOON_MAP = {
    "ãã‚ƒ":"kya","ãã‚…":"kyu","ãã‚‡":"kyo",
    "ãã‚ƒ":"gya","ãã‚…":"gyu","ãã‚‡":"gyo",
    "ã—ã‚ƒ":"sha","ã—ã‚…":"shu","ã—ã‚‡":"sho",
    "ã˜ã‚ƒ":"ja","ã˜ã‚…":"ju","ã˜ã‚‡":"jo",
    "ã¡ã‚ƒ":"cha","ã¡ã‚…":"chu","ã¡ã‚‡":"cho",
    "ã«ã‚ƒ":"nya","ã«ã‚…":"nyu","ã«ã‚‡":"nyo",
    "ã²ã‚ƒ":"hya","ã²ã‚…":"hyu","ã²ã‚‡":"hyo",
    "ã³ã‚ƒ":"bya","ã³ã‚…":"byu","ã³ã‚‡":"byo",
    "ã´ã‚ƒ":"pya","ã´ã‚…":"pyu","ã´ã‚‡":"pyo",
    "ã¿ã‚ƒ":"mya","ã¿ã‚…":"myu","ã¿ã‚‡":"myo",
    "ã‚Šã‚ƒ":"rya","ã‚Šã‚…":"ryu","ã‚Šã‚‡":"ryo",
    "ãµã":"fa","ãµãƒ":"fi","ãµã‡":"fe","ãµã‰":"fo",
}

BASIC_MAP = {
    "ã‚":"a","ã„":"i","ã†":"u","ãˆ":"e","ãŠ":"o",
    "ã‹":"ka","ã":"ki","ã":"ku","ã‘":"ke","ã“":"ko",
    "ãŒ":"ga","ã":"gi","ã":"gu","ã’":"ge","ã”":"go",
    "ã•":"sa","ã—":"shi","ã™":"su","ã›":"se","ã":"so",
    "ã–":"za","ã˜":"ji","ãš":"zu","ãœ":"ze","ã":"zo",
    "ãŸ":"ta","ã¡":"chi","ã¤":"tsu","ã¦":"te","ã¨":"to",
    "ã ":"da","ã¢":"ji","ã¥":"zu","ã§":"de","ã©":"do",
    "ãª":"na","ã«":"ni","ã¬":"nu","ã­":"ne","ã®":"no",
    "ã¯":"ha","ã²":"hi","ãµ":"fu","ã¸":"he","ã»":"ho",
    "ã°":"ba","ã³":"bi","ã¶":"bu","ã¹":"be","ã¼":"bo",
    "ã±":"pa","ã´":"pi","ã·":"pu","ãº":"pe","ã½":"po",
    "ã¾":"ma","ã¿":"mi","ã‚€":"mu","ã‚":"me","ã‚‚":"mo",
    "ã‚„":"ya","ã‚†":"yu","ã‚ˆ":"yo",
    "ã‚‰":"ra","ã‚Š":"ri","ã‚‹":"ru","ã‚Œ":"re","ã‚":"ro",
    "ã‚":"wa","ã‚’":"wo","ã‚“":"N",
}

def kata_to_hira(text: str) -> str:
    """ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›"""
    result = ""
    for ch in text:
        if "ã‚¡" <= ch <= "ãƒ´":
            result += chr(ord(ch) - 0x60)
        else:
            result += ch
    return result

def kana_to_sofa_tokens(text: str) -> List[str]:
    """ã‹ãªæ–‡å­—åˆ—ã‚’SOFAè¾æ›¸ãƒ­ãƒ¼ãƒå­—ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ï¼ˆãƒ¢ãƒ¼ãƒ©å˜ä½ã§åˆ†å‰²ï¼‰"""
    # å‰å‡¦ç†ï¼šã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–ã€ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãª
    text = re.sub(r"\s+", " ", text.strip())
    text = kata_to_hira(text)
    
    tokens = []
    
    # å˜èªã”ã¨ã«å‡¦ç†ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‰ã‚ŒãŸéƒ¨åˆ†ï¼‰
    for word in text.split(" "):
        if not word:
            continue
            
        i = 0
        while i < len(word):
            ch = word[i]
            
            # ä¿ƒéŸ³ã€Œã£ã€â†’ cl
            if ch == "ã£":
                tokens.append("cl")
                i += 1
                continue
                
            # é•·éŸ³ã€Œãƒ¼ã€â†’ ç›´å‰ãƒˆãƒ¼ã‚¯ãƒ³ã®æ¯éŸ³ã‚’ç¹°ã‚Šè¿”ã—
            if ch == "ãƒ¼":
                if tokens and re.search(r"[aiueo]$", tokens[-1]):
                    # ç›´å‰ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ«å°¾æ¯éŸ³ã‚’1ãƒ¢ãƒ¼ãƒ©è¿½åŠ 
                    tokens.append(tokens[-1][-1])
                i += 1
                continue
                
            # æ‹—éŸ³å‡¦ç†ï¼ˆã‚ƒã‚…ã‚‡ï¼‰
            if i + 1 < len(word) and word[i + 1] in "ã‚ƒã‚…ã‚‡":
                pair = word[i:i + 2]
                if pair in YOON_MAP:
                    tokens.append(YOON_MAP[pair])
                    i += 2
                    continue
                    
            # åŸºæœ¬ã‹ãªå‡¦ç†
            if ch in BASIC_MAP:
                tokens.append(BASIC_MAP[ch])
                i += 1
                continue
                
            # ãã®ä»–ã®æ–‡å­—ã¯ç„¡è¦–
            print(f"æœªå¯¾å¿œæ–‡å­—ã‚’ã‚¹ã‚­ãƒƒãƒ—: {ch}")
            i += 1
    
    return tokens  # ãƒ¢ãƒ¼ãƒ©ã ã‘ã®é…åˆ—ã‚’è¿”ã™ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã¯å…¥ã‚Œãªã„ï¼‰

def load_dictionary_keys(dict_path: str) -> set:
    """è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
    keys = set()
    try:
        with open(dict_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if '\t' in line:
                    key = line.split('\t', 1)[0].strip()
                    keys.add(key)
    except Exception as e:
        print(f"è¾æ›¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    return keys

def check_dictionary_coverage(tokens: List[str], dict_keys: set) -> List[str]:
    """ãƒˆãƒ¼ã‚¯ãƒ³ã®è¾æ›¸ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒã‚§ãƒƒã‚¯"""
    unknown_tokens = [token for token in tokens if token not in dict_keys]
    return unknown_tokens

class SOFARunner:
    """SOFAã‚’å®Ÿè¡Œã—ã¦TextGridã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, sofa_dir: str = "SOFA"):
        self.sofa_dir = Path(sofa_dir)
        # çµ¶å¯¾ãƒ‘ã‚¹ã§æŒ‡å®š
        current_dir = Path.cwd()
        self.checkpoint_path = current_dir / self.sofa_dir / "ckpt/SOFA_model_JPN_Ver0.0.2_Beta/japanese-v2.0-45000.ckpt"
        self.dictionary_path = current_dir / self.sofa_dir / "ckpt/SOFA_model_JPN_Ver0.0.2_Beta/japanese-dictionary.txt"
        
    def create_lab_file(self, text: str, output_path: str) -> bool:
        """ã‹ãªâ†’SOFAè¾æ›¸ãƒ­ãƒ¼ãƒå­—ã«å¤‰æ›ã—ã¦.labãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        try:
            # ã‹ãªâ†’ãƒ­ãƒ¼ãƒå­—ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›
            tokens = kana_to_sofa_tokens(text)
            token_string = " ".join(tokens)
            
            print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
            print(f"å¤‰æ›å¾Œãƒˆãƒ¼ã‚¯ãƒ³: {token_string}")
            
            # è¾æ›¸ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
            dict_keys = load_dictionary_keys(str(self.dictionary_path))
            unknown_tokens = check_dictionary_coverage(tokens, dict_keys)
            
            print(f"è¾æ›¸ç™»éŒ²æ¸ˆã¿ã‚­ãƒ¼æ•°: {len(dict_keys)}")
            print(f"ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}")
            print(f"æœªçŸ¥ãƒˆãƒ¼ã‚¯ãƒ³: {unknown_tokens[:10]}")  # æœ€åˆã®10å€‹ã ã‘è¡¨ç¤º
            
            if unknown_tokens:
                print(f"âš ï¸ è¾æ›¸ã«æœªç™»éŒ²ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒ {len(unknown_tokens)} å€‹ã‚ã‚Šã¾ã™")
            else:
                print("âœ… å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¾æ›¸ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™")
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(token_string + "\n")
            return True
        except Exception as e:
            print(f"labãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_sofa_alignment(self, wav_path: str, text: str, output_dir: str) -> Optional[str]:
        """SOFAã‚’å®Ÿè¡Œã—ã¦éŸ³ç´ ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’è¡Œã†"""
        try:
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
            wav_name = Path(wav_path).name
            target_wav = output_path / wav_name
            shutil.copy2(wav_path, target_wav)
            
            # .labãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            lab_name = Path(wav_path).stem + ".lab"
            lab_path = output_path / lab_name
            if not self.create_lab_file(text, str(lab_path)):
                return None
            
            # SOFAã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ã§æŒ‡å®šï¼‰
            cmd = [
                sys.executable, str(self.sofa_dir / "infer.py"),
                "--ckpt", str(self.checkpoint_path),
                "--folder", str(output_path),
                "--g2p", "Dictionary",
                "--dictionary", str(self.dictionary_path),
                "--out_formats", "textgrid,trans",
                "--mode", "force"
            ]
            
            print(f"SOFAå®Ÿè¡Œä¸­: {' '.join(cmd)}")
            print(f"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {Path.cwd()}")
            
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True
            )
            
            if result.returncode == 0:
                print("SOFAå®Ÿè¡Œå®Œäº†!")
                # TextGridãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
                textgrid_files = list(output_path.rglob("*.TextGrid"))
                
                if textgrid_files:
                    # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸTextGridãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™
                    found_textgrid = textgrid_files[0]
                    print(f"TextGridãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {found_textgrid}")
                    return str(found_textgrid)
                else:
                    print(f"TextGridãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:")
                    print(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_path}")
                    print(f"  ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹:")
                    for item in output_path.rglob("*"):
                        print(f"    {item}")
                    return None
            else:
                print(f"SOFAã‚¨ãƒ©ãƒ¼: {result.stderr}")
                print(f"æ¨™æº–å‡ºåŠ›: {result.stdout}")
                return None
                
        except Exception as e:
            print(f"SOFAå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return None

class TextGridParser:
    """TextGridãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, textgrid_path: str):
        self.textgrid_path = textgrid_path
        self.content = self._load_textgrid()
        
    def _load_textgrid(self) -> str:
        """TextGridãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        with open(self.textgrid_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def parse_tier(self, tier_name: str) -> List[Dict]:
        """æŒ‡å®šã•ã‚ŒãŸtierã®æƒ…å ±ã‚’æŠ½å‡º"""
        lines = self.content.split('\n')
        in_target_tier = False
        intervals = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # ç›®çš„ã®tierã®é–‹å§‹ã‚’æ¤œå‡º
            if f'name = "{tier_name}"' in line:
                in_target_tier = True
                continue
            
            # æ¬¡ã®tierã®é–‹å§‹ã§çµ‚äº†
            if in_target_tier and line.startswith('item [') and i+1 < len(lines) and 'class = "IntervalTier"' in lines[i+1]:
                break
            
            # intervalså†…ã®å‡¦ç†
            if in_target_tier and 'intervals [' in line:
                interval_match = re.search(r'intervals \[(\d+)\]:', line)
                if interval_match:
                    interval_num = int(interval_match.group(1))
                    
                    # xmin, xmax, textã‚’å–å¾—
                    xmin = None
                    xmax = None
                    text = None
                    
                    j = i + 1
                    while j < len(lines) and not lines[j].strip().startswith('intervals ['):
                        if 'xmin =' in lines[j]:
                            xmin = float(lines[j].split('=')[1].strip())
                        elif 'xmax =' in lines[j]:
                            xmax = float(lines[j].split('=')[1].strip())
                        elif 'text =' in lines[j]:
                            text = lines[j].split('=')[1].strip().strip('"')
                        j += 1
                    
                    if xmin is not None and xmax is not None and text is not None:
                        intervals.append({
                            'xmin': xmin,
                            'xmax': xmax,
                            'text': text,
                            'duration': xmax - xmin
                        })
        
        return intervals
    
    def get_phoneme_info(self) -> List[Dict]:
        """éŸ³ç´ æƒ…å ±ã‚’å–å¾—"""
        return self.parse_tier("phones")
    
    def get_word_info(self) -> List[Dict]:
        """å˜èªæƒ…å ±ã‚’å–å¾—"""
        return self.parse_tier("words")

def create_timestamps(wav_path: str, lyrics_text: str) -> Tuple[Dict, Optional[str]]:
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ­Œè©ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä½œæˆ
    
    Args:
        wav_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        lyrics_text: æ­Œè©ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        timestamps: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æƒ…å ±ã®è¾æ›¸
        textgrid_path: ç”Ÿæˆã•ã‚ŒãŸTextGridãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä¸€æ™‚çš„ã«ã‚³ãƒ”ãƒ¼ã•ã‚ŒãŸã‚‚ã®ï¼‰
    """
    try:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§SOFAã‚’å®Ÿè¡Œ
        with tempfile.TemporaryDirectory() as temp_dir:
            # SOFAå®Ÿè¡Œ
            sofa_runner = SOFARunner()
            textgrid_path = sofa_runner.run_sofa_alignment(wav_path, lyrics_text, temp_dir)
            
            if not textgrid_path:
                raise Exception("SOFAå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # TextGridè§£æ
            parser = TextGridParser(textgrid_path)
            phoneme_intervals = parser.get_phoneme_info()
            word_intervals = parser.get_word_info()
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æƒ…å ±ã‚’æ§‹ç¯‰
            timestamps = {
                "words": [],
                "phonemes": [],
                "word_durations": []
            }
            
            # å˜èªæƒ…å ±ã‚’å‡¦ç†
            for interval in word_intervals:
                text = interval['text']
                start = interval['xmin']
                end = interval['xmax']
                duration = interval['duration']
                
                timestamps["words"].append((start, end, text))
                
                # SPï¼ˆç„¡éŸ³ï¼‰ã‚„APï¼ˆæ¯ç¶™ãï¼‰ä»¥å¤–ã®æœ‰åŠ¹ãªå˜èªã®æŒç¶šæ™‚é–“ã‚’è¨˜éŒ²
                if text not in ['SP', 'AP', ''] and duration > 0.01:
                    timestamps["word_durations"].append(duration)
            
            # éŸ³ç´ æƒ…å ±ã‚’å‡¦ç†
            for interval in phoneme_intervals:
                phoneme = interval['text']
                start = interval['xmin']
                end = interval['xmax']
                
                timestamps["phonemes"].append((start, end, phoneme))
            
            print(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä½œæˆå®Œäº†:")
            print(f"  å˜èªæ•°: {len(timestamps['words'])}")
            print(f"  éŸ³ç´ æ•°: {len(timestamps['phonemes'])}")
            print(f"  æœ‰åŠ¹ãªå˜èªæŒç¶šæ™‚é–“æ•°: {len(timestamps['word_durations'])}")
            
            # TextGridãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«ã‚³ãƒ”ãƒ¼ï¼ˆä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå‰Šé™¤ã•ã‚Œã‚‹å‰ã«ï¼‰
            if textgrid_path and Path(textgrid_path).exists():
                # ä¸€æ™‚çš„ãªã‚³ãƒ”ãƒ¼å…ˆã‚’ä½œæˆ
                temp_textgrid = tempfile.NamedTemporaryFile(suffix='.TextGrid', delete=False)
                temp_textgrid.close()
                shutil.copy2(textgrid_path, temp_textgrid.name)
                return timestamps, temp_textgrid.name
            else:
                return timestamps, None
            
    except Exception as e:
        print(f"ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return {}, None

import pykakasi

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
    target_dir = "dataset/ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—/raw"
    wav_files = list(Path(target_dir).glob("*.wav"))
    txt_files = list(Path(target_dir).glob("*.txt"))

    if not wav_files or not txt_files:
        print(f"âŒ WAVã¾ãŸã¯TXTãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_dir}")
        return

    wav_path = str(wav_files[0])
    text_path = str(txt_files[0])

    # text.txt ã‚’èª­ã¿è¾¼ã¿ã€pykakasiã§ã²ã‚‰ãŒãªã«å¤‰æ›
    kakasi = pykakasi.kakasi()
    kakasi.setMode("J", "H")  # æ¼¢å­—â†’ã²ã‚‰ãŒãª
    kakasi.setMode("K", "H")  # ã‚«ã‚¿ã‚«ãƒŠâ†’ã²ã‚‰ãŒãª
    kakasi.setMode("H", "H")  # ã²ã‚‰ãŒãªã¯ãã®ã¾ã¾
    conv = kakasi.getConverter()

    raw_text = Path(text_path).read_text(encoding="utf-8").strip()
    lyrics_text = conv.do(raw_text)

    print("=== éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ­Œè©ã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä½œæˆ ===")
    print(f"å…¥åŠ›WAV: {wav_path}")
    print(f"å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {raw_text}")
    print(f"ã²ã‚‰ãŒãªå¤‰æ›å¾Œ: {lyrics_text}")
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä½œæˆ
    timestamps, textgrid_path = create_timestamps(wav_path, lyrics_text)
    
    if timestamps:
        print("\nâœ… ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä½œæˆæˆåŠŸ!")
        
        # ä¸­é–“ç”Ÿæˆç‰©ã‚’ä¿å­˜ï¼ˆif __name__ == "__main__"æ™‚ã®ã¿ï¼‰
        item_name = Path(wav_path).stem.replace(' ', '_').replace('(', '').replace(')', '')
        output_dir = Path("output") / item_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        timestamps_path = output_dir / "a_timestamps.json"
        with open(timestamps_path, 'w', encoding='utf-8') as f:
            json.dump(timestamps, f, indent=2, ensure_ascii=False)
        
        # TextGridãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if textgrid_path and Path(textgrid_path).exists():
            textgrid_output_path = output_dir / "a_textgrid.TextGrid"
            shutil.copy2(textgrid_path, textgrid_output_path)
            print(f"ğŸ“ TextGridæˆæœç‰©ã‚’ä¿å­˜: {textgrid_output_path}")
        else:
            print(f"âš ï¸  TextGridãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {textgrid_path}")
        
        print(f"ğŸ“ JSONæˆæœç‰©ã‚’ä¿å­˜: {timestamps_path}")
        print(f"ğŸ“Š å˜èªæŒç¶šæ™‚é–“ï¼ˆæœ€åˆã®10å€‹ï¼‰: {timestamps['word_durations'][:10]}")
        print(f"ğŸ”— æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: python module/sub/b_convert_to_rosvot.py")
        
        return timestamps
    else:
        print("\nâŒ ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä½œæˆå¤±æ•—")
        return None

if __name__ == "__main__":
    main()
